{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a671bdeb",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "*Author:* Thomas Adler / Eric Volkmann\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990f0c5-56d3-4a70-99af-b6ad9a1dce24",
   "metadata": {},
   "source": [
    "## Warmup: Recap on probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d4be07",
   "metadata": {},
   "source": [
    "### Exercise 1: Bayes' Theorem\n",
    "\n",
    "Prove Bayes' theorem that says\n",
    "\\begin{equation*}\n",
    "    p(X \\mid Y) = \\frac{p(Y \\mid X) p(X)}{p(Y)}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9192da0",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "For two events the equation holds:\n",
    "\\begin{equation*}\n",
    "P(X \\cap Y) = P(X)\\cdot P(Y \\mid X)\n",
    "\\end{equation*}\n",
    "As the following is also true: \n",
    "\\begin{equation*}\n",
    "P(X \\cap Y) =  P(Y) \\cdot P(X \\mid Y)  \n",
    "\\end{equation*}\n",
    "this leads to: \n",
    "\\begin{equation*}\n",
    "P(Y) \\cdot P(X \\mid Y) = P(X)\\cdot P(Y \\mid X)\n",
    "\\end{equation*}\n",
    "if this equation is reformed we get: \n",
    "\\begin{equation*}\n",
    "P(X \\mid Y) = \\frac{P(Y \\mid X) \\cdot P(X)}{P(Y)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1908116",
   "metadata": {},
   "source": [
    "### Exercise 2: Expected Value of the Uniform Distribution\n",
    "\n",
    "Let $X$ be a random variable drawn from the continuous uniform distribution on the interval $[a, b) \\subset \\mathbb R$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf30da-ef10-4918-a79d-994dd5c61b81",
   "metadata": {},
   "source": [
    "a) Calculate the expected value $\\mathbb E[X]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419aa08b",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb E[X] = \\frac{(a +b)} {2}\n",
    "\\end{equation*}\n",
    "\n",
    "as $[a, b) \\subset \\mathbb R$ are uniformly disturbed, \n",
    "\n",
    "\\begin{equation*}\n",
    "    f(x) = \\begin{cases} \n",
    "                \\frac{1}{b - a} & \\text{for } a \\leq x < b \\\\\n",
    "                0 & \\text{else} \n",
    "            \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "The expected value is defined as: \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb E(X) = \\int_{-\\infty}^{\\infty} x f(x) dx.\n",
    "\\end{equation*}\n",
    "\n",
    "This integral arround the interval of $[a, b)$ is zero, so the expected value becomes, \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb E(X) = \\int_{a}^{b} x \\frac{1}{b - a} dx \n",
    "\\end{equation*}\n",
    "\n",
    "As the part $\\frac{1}{b - a}$ is not dependent on x the integral becomes \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb E(X) =  \\frac{1}{b - a} \\int_{a}^{b} x  dx \n",
    "\\end{equation*}\n",
    "\n",
    "Solving the integral we get: \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\int_{a}^{b} x , dx = \\left[ \\frac{x^2}{2} \\right]_{a}^{b} = \\frac{b^2}{2} - \\frac{a^2}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore, E(X) becomes\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb E(X) =  \\frac{1}{b - a} \\cdot \\frac{b^2}{2} - \\frac{a^2}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "Simplify this is getting to the soution of the expected value of a static uniform distribution on the interval $[a, b)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "E(X) = \\frac{(b - a)(b + a)}{2(b - a)} = \\frac{a + b}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e95e4f",
   "metadata": {},
   "source": [
    "b) Calculate the Variance $ \\operatorname{Var}(X) = \\mathbb E \\left [ \\left( X - \\mathbb E [X] \\right)^2 \\right]$.\n",
    "\n",
    "What are the interval boundaries of a centered continuous uniform random variable with unit variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4312c7b2",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5765a",
   "metadata": {},
   "source": [
    "### Exercise 3: CLT Simulation\n",
    "\n",
    "Write a python routine that demonstrates the central limit theorem (CLT). \n",
    "Draw $n$ samples $X_1, \\dots, X_n$ from the uniform distribution over $[-\\sqrt{3},\\sqrt{3})$. \n",
    "Then compute $Z = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i$. \n",
    "Compute 100000 realizations of $Z$ and summarize them in a density or histogram plot. \n",
    "Compare the resulting curves to the density of the standard normal distribution. \n",
    "Perform this procedure for $n \\in \\{1, 2, 10\\}$. \n",
    "What do you observe and how do your observations relate to the CLT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089e9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f40995-bf5c-46db-bfac-de6a341e94e7",
   "metadata": {},
   "source": [
    "## Statistical Learning Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f9f63-d537-4781-8323-e0a7f35a89e5",
   "metadata": {},
   "source": [
    "### Exercise 4: Chernoff Bound for Gaussians with Equal Covariance\n",
    "\n",
    "Read up on section 5.1 \"error bounds for a Gaussian classification task\" in the old lecture notes. \n",
    "Derive the Chernoff bound under the assumption that both classes have equal covariance, i.e., $\\Sigma_1 = \\Sigma_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8773f7-90ad-4ba7-a75d-c5905db56fc6",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f893ad-2de3-4d5c-8fdc-83bc3a0c917f",
   "metadata": {},
   "source": [
    "### Exercise 5: Proof of Markov's Inequality\n",
    "\n",
    "Markov's inequality states that for any non-negative random variable $X$ with finite expectation, \n",
    "\\begin{align*}\n",
    "    \\mathbb P(X \\geq a) \\leq \\frac{\\mathbb E[X]}{a}\n",
    "\\end{align*}\n",
    "holds for all $a > 0$. Prove this result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce83ede-0bd0-4139-8837-133e09a2ec8e",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec68ff3-9088-430b-b1f6-706c626909c3",
   "metadata": {},
   "source": [
    "### Exercise 6: Generalization Bound for a Finite Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01d933-554f-4b4c-a696-eecbb4550529",
   "metadata": {},
   "source": [
    "#### Reminder: \n",
    "Hoeffding's lemma states that\n",
    "\\begin{align*}\n",
    "    \\mathbb E[e^{s(X-\\mathbb E[X])}] \\leq e^{s^2 (b-a)^2 / 8} \\quad \\forall s \\in \\mathbb R,\n",
    "\\end{align*}\n",
    "where $X$ is a random variable bounded by $a \\leq X \\leq b$. \n",
    "\n",
    "Hoeffding's inequality states the following:\n",
    "\n",
    "Let $X_1, \\dots, X_n$ be $n$ independent random variables bounded by $a \\leq X_i \\leq b$ for all $i \\in \\{1, \\dots, n\\}$ and let $Z = \\sum_{i=1}^n X_i$. \n",
    "Then\n",
    "\\begin{align*}\n",
    "    \\mathbb P(Z - \\mathbb E[Z] \\geq t) \\leq \\exp\\left(-\\frac{2t^2}{n(b-a)^2}\\right)\n",
    "\\end{align*}\n",
    "holds for all $s,t > 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c3f16-5f79-4aa5-b470-67d3ecd96df9",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Let $R(g)$ denote the risk and $\\hat R(g)$ the empirical risk over a training set of $n$ samples for a model $g \\in \\mathcal G$, where the model class $\\mathcal G$ consists of $m < \\infty$ models. \n",
    "Both $R$ and $\\hat R$ are based on the zero-one loss, i.e., $R(g) = \\mathbb E[\\ell(Y, g(X))]$ and $\\hat R = \\frac1n \\sum_{i=1}^n \\ell(y_i, g(x_i))$, where $\\ell: \\mathcal Y \\times \\mathcal Y \\to \\{0,1\\}$ is the zero-one loss function. \n",
    "Further, let $\\hat g = \\arg \\min_{g \\in \\mathcal G} \\hat R(g)$ be the model that minimizes the empirical risk and $g^\\ast = \\arg \\min_{g \\in \\mathcal G} R(g)$ the model that minimizes the risk. \n",
    "\n",
    "Prove the following bound on the estimation error $R(\\hat g) - R(g^\\ast)$ (i.e., the excess of the risk due to empirical risk minimization) using Hoeffding's inequality:\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}(R(\\hat g) - R(g^\\ast) \\geq t) \\leq 2 m \\exp(-\\frac{nt^2}{2}).\n",
    "\\end{align*}\n",
    "Interpret this result!\n",
    "\n",
    "*Hint 1*: Prove and use the fact that $R(\\hat g) - R(g^\\ast) \\leq 2 \\max_{g \\in \\mathcal G} |\\hat R(g) - R(g)|$.\n",
    "\n",
    "*Hint 2*: Compute Hoeffdings inequality for the expression $\\mathbb P(|Z - \\mathbb E[Z]| \\geq t)$ and try to use this new inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a0c8a-2510-4fba-8ed8-6a8f2d0faa6e",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434d118-317c-455a-9d7f-39aa66463156",
   "metadata": {},
   "source": [
    "### Exercise 7: Generalization Bound for Floating Point Models\n",
    "\n",
    "The bound from the previous exercise applies only to finite model classes. \n",
    "When we optimize a model over real parameters on a computer, the model class (although theoretically infinite) is practically restricted by the representation of parameters as floating-point numbers. \n",
    "Modify the bound from the previous exercise to a model class with $k$ parameters represented by $b$ bits floating-point numbers. \n",
    "For a risk excess of $t = 0.01$ and a risk excess probability of not more than $\\delta = 0.01$ and using 32-bit parameters, what is the required training set size $n$ as a function of the number of parameters $k$? \n",
    "Discuss your results! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7c06f-6442-4bb6-8e2e-dd55bdc9c2ed",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112a369-0bad-48de-868e-ce48d76992e8",
   "metadata": {},
   "source": [
    "### Exercise 8: VC Dimension of Linear Classifiers\n",
    "\n",
    "The VC dimension is defined as \n",
    "\\begin{align*}\n",
    "    \\operatorname{VC}(\\mathcal G) = \\max_{n \\in \\mathbb N} \\{n \\mid \\sup_{x_1, \\dots, x_n} N_{\\mathcal G}(x_1, \\dots, x_n) = 2^n\\},\n",
    "\\end{align*}\n",
    "where $N_{\\mathcal G}$ is the shattering coefficient, i.e., the number of different binary labellings the model class $\\mathcal G$ can produce for the set $\\{x_1, \\dots, x_n\\}$. \n",
    "Intuitively, the VC dimension is the maximum number of points for which there exists a configuration so that $\\mathcal G$ can produce all possible labellings. \n",
    "\n",
    "Consider the model class \n",
    "\\begin{align*}\n",
    "    \\mathcal G = \\{g : g(x) = \\operatorname{sign}(w^\\top x), w\\in \\mathbb R^d\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875bda4-deeb-4bab-b00d-9ad2d97d9e89",
   "metadata": {},
   "source": [
    "a) Lower bound:  Prove that $\\operatorname{VC}(\\mathcal G) \\geq d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8c8f7-b9df-4179-bfe0-49e77010c538",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c91a70-32c1-4832-9bd1-c0f2c8864abe",
   "metadata": {},
   "source": [
    "b) Upper bound: Prove that $\\operatorname{VC}(\\mathcal G) < d+1$ and conclude that $\\operatorname{VC}(\\mathcal G) = d$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60534c25-7979-49f5-846a-0f5498ca827b",
   "metadata": {},
   "source": [
    "*Hint:* You can do a proof by contradiction. \n",
    "To this end, assume that $\\operatorname{VC}(\\mathcal G) = d+1$. \n",
    "This implies that there exist $2^{d+1}$ models $w$ generating all possible labellings. \n",
    "Assume we assemble these models into one matrix $W \\in \\mathbb R^{d \\times 2^{d+1}}$ whose columns contain the $2^{d+1}$ parameter vectors. \n",
    "Then the matrix $X^\\top W \\in \\mathbb{R}^{(d+1) \\times 2^{d+1}}$ must contain all possible labellings in its columns, where $X = (x_1, \\dots, x_{d+1}) \\in \\mathbb R^{d \\times d+1}$. \n",
    "Explain why such a matrix cannot exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2da0b4-ee31-41bd-ba26-67743b0c8830",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
