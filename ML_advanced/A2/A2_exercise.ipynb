{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8280b461",
   "metadata": {},
   "source": [
    "# Assignment 2: \n",
    "\n",
    "*Author:* Thomas Adler, Markus Holzleitner, Eric Volkmann\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da5a54-3cb8-438b-b9fb-7cbf590bbd7f",
   "metadata": {},
   "source": [
    "# Estimation Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f26bcf",
   "metadata": {},
   "source": [
    "## Exercise 1: Fisher Information (2pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a62bd-6ef6-4e79-ae75-b89f7cb99faf",
   "metadata": {},
   "source": [
    "### a) Likelihood (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82e431-e4b4-458e-8d35-021f23c3d244",
   "metadata": {},
   "source": [
    "Suppose we draw $n$ i.i.d samples  $\\{x_1, \\ldots, x_n \\}$ from a random variable $X$ with pmf $f(x; \\theta)$, where $\\theta$ is the parameter. The likelihood function is\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\{x_1, \\ldots, x_n \\}; \\theta) &= \\prod^n_{i=1} f(x_i; \\theta) . \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Assuming you know the Fisher Information $I_F(\\theta)$ of one sample under $f(x; \\theta)$, calculate the Fisher Information $I_F^{\\mathcal L}(\\theta)$ of $n$ iid samples, i.e., of the likelihood function. \n",
    "Assume that all regularity conditions are met for $f$ and $\\mathcal{L}$, i.e., you can use the identities\n",
    "\\begin{equation*}\n",
    "I_F^{\\mathcal L}(\\theta) = - \\mathbb{E}_X \\Big[ \\frac{\\partial^2}{\\partial \\theta^2} \\ln \\mathcal L(\\{x_1, \\ldots, x_n \\}; \\theta) \\Big] \\quad \\text{and} \\quad I_F(\\theta) = - \\mathbb{E}_X \\Big[ \\frac{\\partial^2}{\\partial \\theta^2} \\ln f(x; \\theta) \\Big].\n",
    "\\end{equation*}\n",
    "Interpret the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72be59d",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1d0c9",
   "metadata": {},
   "source": [
    "### b) Bernoulli Distribution (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7a3da-9143-490e-9ec3-91a52bcca61f",
   "metadata": {},
   "source": [
    "Consider a Bernoulli distributed random variable $X$ with probability mass function (pmf)\n",
    "\\begin{align*}\n",
    "f(x ; p) &= p^x(1-p)^{1-x} , \\\\\n",
    "\\end{align*}\n",
    "where $0 \\leq p \\leq 1$ is the parameter and the support is $x \\in \\{0,1\\}$. \n",
    "Calculate the Fisher information. \n",
    "Assume $f$ fulfills all necessary regularity conditions so that you may use the form\n",
    "\\begin{align*}\n",
    "I_F(p) &= -\\mathbb{E}_X \\Big[ \\frac{d^2}{dp^2} \\ln f(x ; p) \\Big]  \\ .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd1534",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3518c1d",
   "metadata": {},
   "source": [
    "### c) Poisson Distribution (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e0816-5441-4127-a75f-2f3cfe7c237a",
   "metadata": {},
   "source": [
    "Consider a Poisson distributed random variable $X$ with pmf\n",
    "\\begin{align*}\n",
    "p(x ; \\lambda) &= \\frac{\\lambda^x}{x!} e^{-\\lambda} \\ , \\\\ \n",
    "\\end{align*}\n",
    "where $\\lambda$ is the parameter and the support is $x \\in \\mathbb{N} \\cup 0$.\n",
    "Calculate the Fisher information. Again, assume that all necessary regularity conditions hold, i.e. you can use the form\n",
    "\\begin{align*}\n",
    "I_F(\\lambda) &= -\\mathbb{E}_X \\Big[ \\frac{d^2}{d \\lambda^2} \\ln p(x ; \\lambda) \\Big] \\ .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447fb138",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b4f76",
   "metadata": {},
   "source": [
    "### d) Variance of the Normal Distribution (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fe0a9-ab31-4f3d-bc2a-b09eb1987f6e",
   "metadata": {},
   "source": [
    "Consider a normally distributed random variable $X$ with pmf\n",
    "\\begin{align*}\n",
    "p(x ; \\mu, \\sigma^2) &= \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\\\ \n",
    "\\end{align*}\n",
    "where $\\mu$ and $\\sigma^2$ are the parameters and the support is $x \\in \\mathbb R$.\n",
    "Calculate the Fisher information for $\\sigma^2$. Again, assume that all necessary regularity conditions hold, i.e. you can use the form\n",
    "\\begin{align*}\n",
    "I_F(\\lambda) &= -\\mathbb{E}_X \\Big[ \\frac{d^2}{d (\\sigma^2)^2} \\ln p(x ; \\mu, \\sigma^2) \\Big] \\ .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad81f30",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dcbdc1-f4d4-48f6-bce1-de0f69a7c711",
   "metadata": {},
   "source": [
    "## Excercise 2: Properties of estimators (2pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46ba27",
   "metadata": {},
   "source": [
    "### a) The sample average (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f35f9-1e61-41d0-83dd-f493e1d23a67",
   "metadata": {},
   "source": [
    "Given a sequence of iid random variables $X_1, \\dots, X_n$ with variance $\\sigma^2$, their sample average is given by\n",
    "\\begin{align*}\n",
    "    \\bar X = \\frac1n \\sum_{i=1}^n X_i. \n",
    "\\end{align*}\n",
    "Is it a biased or unbiased estimator of the mean? Calculate the variance of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474b030",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73202973",
   "metadata": {},
   "source": [
    "### b) The sample variance (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff335a0-4b67-4aee-9350-3e29e86aeaa1",
   "metadata": {},
   "source": [
    "Consider an iid sequence of random variables $X_1, \\dots, X_n$ with unknown mean $\\mu$ and variance $\\sigma^2$. \n",
    "An estimator for the variance is given by the sample variance:\n",
    "\\begin{equation*}\n",
    "    \\hat \\sigma^2 = \\frac1n \\sum_{i=1}^n (X_i - \\bar X)^2. \n",
    "\\end{equation*}\n",
    "Show that this is a biased estimator of $\\sigma^2$. Can you correct it? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f78b5",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38f715-7dc2-4430-b971-77668cfdbb65",
   "metadata": {},
   "source": [
    "### c) Maximum Likelihood Estimator for the Variance of a Gaussian (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a914bc-9b89-4582-91b4-cc66e82379c2",
   "metadata": {},
   "source": [
    "Derive the maximum likelihood estimator for the variance of $n$ data points $x_1, \\dots, x_n$ drawn independently from a normal distribution. \n",
    "Is this estimator biased?\n",
    "*Hint: use the log-likelihood for your derivation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305381f-4894-4558-b1be-791068ba5f04",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48cf06",
   "metadata": {},
   "source": [
    "### d) Variance of the Variance Estimator (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a8a45c-a303-43ab-a52f-47fad9b1075b",
   "metadata": {},
   "source": [
    "Write a python routine that estimates the variance of the estimator $\\hat \\sigma^2$ using a sample of $n=10$ standard normally distributed variables over 10000 trials. \n",
    "Then do the same for the bias-corrected version of $\\hat \\sigma^2$. \n",
    "Compare both estimates to the Cramer-Rao lower bound and interepret and explain the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97774767",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1af5d1-d225-4914-aff6-49616b9147be",
   "metadata": {},
   "source": [
    "# Probabilistic PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2bb62-a345-4a0b-9483-2f77d07c90f6",
   "metadata": {},
   "source": [
    "Probabilistic Principal Component Analysis or pPCA for short makes the following assumptions. \n",
    "We have observable variables $x \\in \\mathbb R^d$ and latent variables $z \\in \\mathbb R^m$, where $m < d$. \n",
    "We model $z \\sim \\mathcal N(0, I)$ and $x \\mid z \\sim \\mathcal N(W z + b, \\sigma^2 I)$, where $\\sigma^2 \\in \\mathbb R_+$ and $I$ is the identity matrix of appropriate size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88ea54-f90a-4d17-a300-6d8b21680630",
   "metadata": {},
   "source": [
    "The original paper introducing pPCA can be found here: https://www.cs.columbia.edu/~blei/seminar/2020-representation/readings/TippingBishop1999.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e161f9-1e09-4358-8d2a-33dcdf88947f",
   "metadata": {},
   "source": [
    "## Exercise 3: Moment Generating Functions (MGF) (1pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b602f8-d6e6-4f01-a6bb-9d1bc56c594d",
   "metadata": {},
   "source": [
    "### a) MGF of $W \\xi + b$ (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669eb3a-e964-46cf-9876-ebd29490b0fa",
   "metadata": {},
   "source": [
    "The MGF of a random vector $\\xi \\in \\mathbb R^m$ is defined as \n",
    "\\begin{align*}\n",
    "    M_{\\xi}(t) = \\mathbb E[\\exp(t^\\top \\xi)],\n",
    "\\end{align*}\n",
    "where $t \\in \\mathbb R^m$. \n",
    "Show that the MGF of $W \\xi + b$ is\n",
    "\\begin{align*}\n",
    "    M_{W \\xi + b}(t) &= \\exp(t^\\top b) M_{\\xi}(W^\\top t). \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d9013-f7eb-47ff-a7ca-5390d35a2eca",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508fa0a6-e17a-4946-b0a3-da9f6b5e96a2",
   "metadata": {},
   "source": [
    "### b) Linear Combination of Gaussians (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a533d34-07c3-4093-b23b-dfb8d12a6144",
   "metadata": {},
   "source": [
    "If $\\xi$ is normally distributed with mean $\\mu$ and covariance matrix $\\Sigma$, then its MGF is given by\n",
    "\\begin{align*}\n",
    "    M_{\\xi}(t) = \\exp(t^\\top(\\mu + \\frac12 \\Sigma t)). \n",
    "\\end{align*}\n",
    "Use this fact and the result from the previous exercise to show that the marginal $x \\sim \\mathcal N(b, WW^\\top + \\sigma^2 I)$. \n",
    "\n",
    "*Hint: argue that $x$ can be written as $Wz + b + \\varepsilon$, where $z \\sim \\mathcal N(0, I)$ and $\\varepsilon \\sim \\mathcal N(0, \\sigma^2 I)$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08fa885-a631-4145-9547-a7bdfa9b1b49",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3bf04-a1df-4cff-8e7a-cffc9574daa0",
   "metadata": {},
   "source": [
    "## Exercise 4: The Likelihood Function (2pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0006e35-391f-4d5d-8658-c686c2507d75",
   "metadata": {},
   "source": [
    "### a) Maximum Likelihood for $b$ (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffee6dc-0418-439f-9ba5-7995b0b5fa26",
   "metadata": {},
   "source": [
    "Now that we have a closed form for the marginal $p(x) = \\mathcal N(b, WW^\\top + \\sigma^2 I)$ we can formulate the log-likelihood function as \n",
    "\\begin{align*}\n",
    "    \\log \\mathcal L(\\{x_i\\} \\mid b, W, \\sigma^2) &= \\sum_{i=1}^n \\log p(x_i) \\\\\n",
    "    &= -\\frac{nd}{2} \\log(2 \\pi) - \\frac{n}{2} \\log|C| - \\frac12 \\sum_{i=1}^n (x_i - b)^\\top C^{-1} (x - b),\n",
    "\\end{align*}\n",
    "where $C = WW^\\top + \\sigma^2 I$. \n",
    "We would like to maximize it with respect to the parameters $b, W, \\sigma^2$. \n",
    "Prove that the arithmetic mean $b^{\\ast} = \\bar x = \\frac1n \\sum_{i=1}^n x_i$ maximizes the log-likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a266b3-ecfb-4470-96bc-992dab866768",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b4bbc-6f38-407c-87e6-2df6d37e5ba4",
   "metadata": {},
   "source": [
    "### Information: Maximum Likelihood for $W$ (nothing to do here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af9f93-37e4-4fa0-94dc-c1d82738a4db",
   "metadata": {},
   "source": [
    "Substituting the result from the previous exercise back into the log-likelihood function lets us rewrite it as\n",
    "\\begin{align*}\n",
    "    \\log \\mathcal L(\\{x_i\\} \\mid b, W, \\sigma^2) &= -\\frac{n}{2} \\left(d \\log(2 \\pi) + \\log|C| + \\operatorname{Tr}(C^{-1} S) \\right),\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "    S = \\frac1n \\sum_{i=1}^n (x_i - \\bar x) (x_i - \\bar x)^\\top \n",
    "\\end{align*}\n",
    "is the sample covariance matrix. \n",
    "The derivative of the log-likelihood function w.r.t. $W$ is given by\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\log \\mathcal L}{\\partial W} &= n(C^{-1} S C^{-1} W - C^{-1} W). \n",
    "\\end{align*}\n",
    "Setting it to zero leads to the condition\n",
    "\\begin{align*}\n",
    "    SC^{-1}W = W.\n",
    "\\end{align*}\n",
    "Assuming $W \\neq 0$ and $S \\neq C$, one can show that\n",
    "\\begin{align*}\n",
    "    SU = U(\\sigma^2 I + D^2)\n",
    "\\end{align*}\n",
    "is a sufficient condition, where $U D V^\\top$ is the singular value decomposition of $W$ with $U \\in \\mathbb R^{d \\times m}$ and $D, V \\in \\mathbb R^{m \\times m}$ and $D$ diagonal. \n",
    "The columns of $U$ are the eigenvectors of $S$ with eigenvalues $\\lambda_i = d_i^2 + \\sigma^2$. The solution may be written as\n",
    "\\begin{align*}\n",
    "    W = U (\\Lambda - \\sigma^2I)^{\\frac12} V^\\top,\n",
    "\\end{align*}\n",
    "where $\\Lambda$ is a $m \\times m$ diagonal matrix holding $m$ eigenvalues of $S$. We assume that all $d < m$ eigenvalues of $S$ are positive. \n",
    "\n",
    "\n",
    "What is the role of $V$? It turns out that $V$ can be any orthogonal matrix. By changing $V$, we select a different basis for the latents $z$. \n",
    "This does not make a difference since $z$ are rotation invariant. For simplicity, we can just select $V=I$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7a05c-f2de-481a-832f-918747c495c8",
   "metadata": {},
   "source": [
    "### b) The Likelihood Function (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3fa5d-34dd-4934-8f49-85e6042ddf30",
   "metadata": {},
   "source": [
    "In the following, let $Y$ be the \"full\" $d \\times d$ eigenbasis of $S$ (in contrast to the \"reduced\" $d \\times m$ matrix $U$). \n",
    "Show that when we substitute our solution for $W = U (\\Lambda - \\sigma^2I)^{\\frac12}$ into $C=WW^\\top + \\sigma^2 I$, we get\n",
    "\\begin{align*}\n",
    "    C &= Y K Y^\\top,\n",
    "\\end{align*}\n",
    "where $K$ is an $d \\times d$ diagonal matrix with \n",
    "\\begin{align*}\n",
    "    k_i = \n",
    "    \\begin{cases}\n",
    "    \\lambda_i \\quad \\text{ if } i \\leq m \\\\\n",
    "    \\sigma^2 \\quad \\text{ else.}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "Use this result to show that the log-likelihood function can be written as\n",
    "\\begin{align*}\n",
    "    \\log \\mathcal L(\\{x_i\\} \\mid b, W, \\sigma^2) &= -\\frac{n}{2} \\left(d \\log(2 \\pi) + \\sum_{i=1}^m \\log \\lambda_i + (d-m) \\log \\sigma^2 + m + \\frac{1}{\\sigma^2} \\sum_{j=m+1}^d \\lambda_j \\right). \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2dec10-cc18-425f-8b4a-8f2d1bc960c6",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b88ba-9c70-48b9-8556-a4f1203bede7",
   "metadata": {},
   "source": [
    "### c) Maximum Likelihood for $\\sigma^2$ (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903133d-6fe0-4eb8-95dd-241f01e315b7",
   "metadata": {},
   "source": [
    "Use the likelihood function derived in the previous exercise to conclude that the maximum likelihood estimator for $\\sigma^2$ is\n",
    "\\begin{align*}\n",
    "    \\sigma^2 &= \\frac{1}{d-m} \\sum_{j=m+1}^d \\lambda_j .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbfb482-afff-49ab-8299-263b34bc1313",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc20592-c16e-4cfe-8fa9-42ba1547f28e",
   "metadata": {},
   "source": [
    "## Exercise 5: Implementation (1pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428eced0-b869-4deb-b3d5-ab5bff5196e9",
   "metadata": {},
   "source": [
    "### a) Generate Toy Data (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c73d4-f252-40ab-ad38-845d6a017cb2",
   "metadata": {},
   "source": [
    "Probabilistic PCA is a generative model, i.e., it comes with a description of the data generation process. \n",
    "In this exercise, let $d=10, m=3$, and $\\sigma^2 = 1/2$. \n",
    "Draw model parameters $W, b$ from a standard normal distribution and generate a dataset $X$ according to the pPCA generative model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e353ec-e51f-49f7-84ab-5ffa4b74d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da4180-b6d8-4f9d-a9c2-90b38d1ddd68",
   "metadata": {},
   "source": [
    "### b) Parameter Estimation by Maximum Likelihood (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0612d-b3d2-42b4-b811-488ec21201f5",
   "metadata": {},
   "source": [
    "Take the data matrix from the previous exercise and fit the parameters $b, \\sigma^2, W$ according to their maximum likelihood estimates. Interpret your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0400801-4385-4b00-975b-1d41606a57e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
