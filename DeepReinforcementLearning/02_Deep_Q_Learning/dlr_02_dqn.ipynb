{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU3jFH0qWPCF"
      },
      "source": [
        "# Setup Notebook (Virtual machine)\n",
        "\n",
        "In this section all required packages will be installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3XE3PLtc1-C"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y swig python3-numpy python3-dev cmake zlib1g-dev libjpeg-dev xvfb ffmpeg xorg-dev python3-opengl libboost-all-dev libsdl2-dev\n",
        "!pip install gymnasium==0.29.0 gymnasium[box2d] pyvirtualdisplay imageio-ffmpeg moviepy==1.0.3\n",
        "!pip install onnx onnx2pytorch==0.4.1\n",
        "!pip install opencv-python pyvirtualdisplay\n",
        "\n",
        "!wget https://raw.githubusercontent.com/StefOe/colab-pytorch-utils/master/utils.py\n",
        "import utils\n",
        "\n",
        "\n",
        "!pip install minigrid==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CHVqukdAdOhm"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cv2'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspaces\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import collections\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import cv2\n",
        "import gymnasium as gym\n",
        "import gymnasium.spaces\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "%matplotlib inline\n",
        "!nvcc --version\n",
        "\n",
        "from matplotlib import animation\n",
        "import seaborn as sns; sns.set()\n",
        "from IPython.display import clear_output, HTML\n",
        "from IPython import display\n",
        "\n",
        "import torch.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBjLMoBzrRkq"
      },
      "source": [
        "# Mount your drive and use GPU\n",
        "\n",
        "Here you can mount your GDrive folders (where you will store checkpoints during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP7nL3c7m6n5"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('/content/gdrive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "897-9ZhZgYJq"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8onrKHdtCif"
      },
      "source": [
        "# Replay Buffer\n",
        "\n",
        "The replay buffer stores transitions - state, action, reward, next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLTkkXrbgYG8"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "class ReplayBuffer():\n",
        "  def __init__(self, num_actions, memory_len = 10000):\n",
        "      self.memory_len = memory_len\n",
        "      self.transition = []\n",
        "      self.num_actions = num_actions\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "      if self.length() > self.memory_len:\n",
        "        self.remove()\n",
        "      self.transition.append(Transition(state, action, reward, next_state, done))\n",
        "\n",
        "  def sample_batch(self, batch_size = 32):\n",
        "      minibatch = random.sample(self.transition, batch_size)\n",
        "      states_mb, a_, reward_mb, next_states_mb, done_mb = map(np.array, zip(*minibatch))\n",
        "\n",
        "      mb_reward = torch.from_numpy(reward_mb).to(device=device, dtype=torch.float32)\n",
        "      mb_done = torch.from_numpy(done_mb.astype(int)).to(device=device)\n",
        "      a_ = a_.astype(int)\n",
        "      a_mb = np.zeros((a_.size, self.num_actions), dtype=np.float32)\n",
        "      a_mb[np.arange(a_.size), a_] = 1\n",
        "      mb_a = torch.from_numpy(a_mb).cuda().to(device=device)\n",
        "      return states_mb, mb_a, mb_reward, next_states_mb, mb_done # states will be converted to tensors in forward pass\n",
        "\n",
        "  def length(self):\n",
        "      return len(self.transition)\n",
        "\n",
        "  def remove(self):\n",
        "      self.transition.pop(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnQOXbjia2_0"
      },
      "source": [
        "# MINIGRID DQN-Training\n",
        "\n",
        "This section contains the train function for the minigrid environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLEOIJlULd6x"
      },
      "source": [
        "### Minigrid Environment and Policy Network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UCSXOrJgYEJ"
      },
      "outputs": [],
      "source": [
        "# Minigrid Environment\n",
        "from minigrid.wrappers import ImgObsWrapper\n",
        "class ChannelFirst(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        old_shape = env.observation_space.shape\n",
        "        self.observation_space = {}\n",
        "        self.observation_space = gym.spaces.Box(0, 255, shape=(3, 7, 7))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.swapaxes(observation, 2, 0)\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32)\n",
        "\n",
        "class MinigridEmpty5x5ImgObs(gym.Wrapper):\n",
        "    \"\"\"Minigrid with image observations provided by minigrid, partially observable.\"\"\"\n",
        "    def __init__(self, render=False):\n",
        "        if render:\n",
        "          env = gym.make('MiniGrid-Empty-5x5-v0', render_mode=\"rgb_array\")\n",
        "        else:\n",
        "          env = gym.make('MiniGrid-Empty-5x5-v0')\n",
        "        env = ScaledFloatFrame(ChannelFirst(ImgObsWrapper(env)))\n",
        "        super().__init__(env)\n",
        "\n",
        "class MinigridDoorKey6x6ImgObs(gym.Wrapper):\n",
        "    \"\"\"Minigrid with image observations provided by minigrid, partially observable.\"\"\"\n",
        "    def __init__(self, render=False):\n",
        "        if render:\n",
        "          env = gym.make('MiniGrid-DoorKey-6x6-v0', render_mode=\"rgb_array\")\n",
        "        else:\n",
        "          env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
        "        env = ScaledFloatFrame(ChannelFirst(ImgObsWrapper(env)))\n",
        "        super().__init__(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY36BedOmGas"
      },
      "outputs": [],
      "source": [
        "env = MinigridDoorKey6x6ImgObs()\n",
        "obs = env.reset()[0]\n",
        "obs, obs.shape, env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swwgl209mGX3"
      },
      "outputs": [],
      "source": [
        "class MlpMinigridPolicy(nn.Module):\n",
        "    def __init__(self, num_actions=7):\n",
        "        super().__init__()\n",
        "        self.num_actions = num_actions\n",
        "        self.fc = nn.Sequential(nn.Flatten(),\n",
        "                                nn.Linear(3*7**2, 256), nn.ReLU(),\n",
        "                                nn.Linear(256, 256), nn.ReLU(),\n",
        "                                nn.Linear(256, 64), nn.ReLU(),\n",
        "                                nn.Linear(64, num_actions))\n",
        "    def forward(self, x):\n",
        "        if len(x.size()) == 3:\n",
        "          x = x.unsqueeze(dim=0)\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv3rBDMva2_7"
      },
      "source": [
        "### Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m---cnKwmGPG"
      },
      "outputs": [],
      "source": [
        "def plot(frame_idx, rewards, losses):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
        "    plt.plot(rewards)\n",
        "    plt.subplot(132)\n",
        "    plt.title('loss')\n",
        "    plt.plot(losses)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCskxg2fmggx"
      },
      "outputs": [],
      "source": [
        "def set_seed(env, seed=None):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        env.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAKPCNDYmgd-"
      },
      "outputs": [],
      "source": [
        "num_episodes = 1000 # number of episodes to run the algorithm -> the algorithm should reach a score of about 0.9 within 1000 episodes\n",
        "buffer_size = 100000 # size of the buffer to use\n",
        "epsilon = 1.0 # initial probablity of selecting random action a, annealed over time\n",
        "timesteps = 0 # counter for number of frames\n",
        "minibatch_size = 5 #128 # size of the minibatch sampled\n",
        "gamma = 0.99 # discount factor\n",
        "eval_episode = 100\n",
        "num_eval = 10\n",
        "tau = 1e-3\n",
        "learning_rate = 0.0001\n",
        "update_after = 2000 # update after num time steps\n",
        "epsilon_decay = 10000 # decay epsilon in 100.000 timesteps\n",
        "epsilon_ub = 1.0\n",
        "epsilon_lb = 0.3\n",
        "# set seed\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZpUnaP1nfrR"
      },
      "outputs": [],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HI0DN4la3AA"
      },
      "source": [
        "### Load model (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9RSKdDPnfo8"
      },
      "outputs": [],
      "source": [
        "# specify load path if available:\n",
        "\n",
        "# load_path = '/content/gdrive/MyDrive/colab-drive/minigrid-model-2022_03_28-10_01_18.p'\n",
        "\n",
        "load_path = '' # otherwise start with randomly initialized agent\n",
        "\n",
        "# save the current model state\n",
        "save_path = f\"/content/gdrive/MyDrive/colab-drive/minigrid-model-{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}.p\"\n",
        "save_path = f\"minigrid-model-{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}.p\"\n",
        "\n",
        "print(f'model checkpoints will be saved to: {save_path}')\n",
        "\n",
        "def load_checkpoint(checkpoint_path='', device=device):\n",
        "  dqn = MlpMinigridPolicy(num_actions=num_actions).to(device=device)\n",
        "  dqn_target = MlpMinigridPolicy(num_actions=num_actions).to(device=device)\n",
        "  timesteps = 0\n",
        "\n",
        "  if checkpoint_path:\n",
        "    print(f'Loading checkpoint {checkpoint_path}')\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location=device)\n",
        "    model_params = checkpoint_dict['model_params']\n",
        "    timesteps = checkpoint_dict['timesteps'] # environment steps\n",
        "\n",
        "    dqn.load_state_dict(model_params) # makes a copy of model_params\n",
        "    dqn_target.load_state_dict(model_params)\n",
        "  else:\n",
        "    print(f'Starting training from scratch.')\n",
        "\n",
        "  return dqn, dqn_target, timesteps\n",
        "\n",
        "def store_checkpoint(checkpoint_path, dqn_net, timesteps):\n",
        "  checkpoint_dict = {'model_params':dqn_net.state_dict(), 'timesteps': timesteps}\n",
        "  torch.save(checkpoint_dict, checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQl9VFfPKbEB"
      },
      "outputs": [],
      "source": [
        "# Initialize environment\n",
        "\n",
        "# VERY IMPORTANT:\n",
        "# Fast environment to build your code: MinigridEmpty5x5ImgObs() <- Train on this to improve your code fast.\n",
        "# Actual environment for the exercise: MinigridDoorKey6x6ImgObs() <- Train on this for submitting on challenge server.\n",
        "# Make sure to train on the right environment, when you are submitting your model.\n",
        "\n",
        "env = MinigridDoorKey6x6ImgObs()\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "state_space = env.observation_space.shape\n",
        "print(num_actions, state_space)\n",
        "obs, obs.shape, env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utf28URPmgjw"
      },
      "outputs": [],
      "source": [
        "# Update Target network\n",
        "def soft_update(local_model, target_model, tau):\n",
        "    \"\"\"Soft update model parameters.\n",
        "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "    Params\n",
        "    ======\n",
        "        local_model (PyTorch model): weights will be copied from\n",
        "        target_model (PyTorch model): weights will be copied to\n",
        "        tau (float): interpolation parameter\n",
        "    \"\"\"\n",
        "    # TODO: Update target network\n",
        "    target_model = tau * local_model + (1- tau)* target_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCtG6K2Fa3AC"
      },
      "source": [
        "### DQN training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBRdR92dnfmI"
      },
      "outputs": [],
      "source": [
        "from torch.serialization import load\n",
        "\n",
        "# Train the agent using DQN for Pong\n",
        "returns = []\n",
        "returns_50 = deque(maxlen=50)\n",
        "losses = []\n",
        "buffer = ReplayBuffer(num_actions=num_actions, memory_len=buffer_size)\n",
        "\n",
        "dqn, dqn_target, timesteps = load_checkpoint(load_path)\n",
        "\n",
        "optimizer = optim.Adam(dqn.parameters(), lr=learning_rate)\n",
        "mse = torch.nn.MSELoss()\n",
        "state = env.reset()[0]\n",
        "for i in range(num_episodes):\n",
        "  ret = 0\n",
        "  done = False\n",
        "  while not done:\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon_lb, epsilon_ub - timesteps/ epsilon_decay)\n",
        "    # action selection\n",
        "    if np.random.choice([0,1], p=[1-epsilon,epsilon]) == 1:\n",
        "      a = np.random.randint(low=0, high=num_actions, size=1)[0]\n",
        "    else:\n",
        "      # state_tmp = state[np.newaxis, :].astype(np.float32)\n",
        "      state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "      net_out = dqn(state_tensor).detach().cpu().numpy()\n",
        "      a = np.argmax(net_out)\n",
        "#    next_state, r, done, info = env.step(a)\n",
        "    next_state, r, terminated, truncated, info = env.step(a)\n",
        "    done = terminated or truncated\n",
        "#    print(next_state.shape)\n",
        "    ret = ret + r\n",
        "    # TODO: store transition in replay buffer\n",
        "    buffer.add(state=state, action=a, reward=r, next_state=next_state, done=done)\n",
        "    state = next_state\n",
        "    timesteps = timesteps + 1\n",
        "\n",
        "    # update policy using temporal difference\n",
        "    if buffer.length() > minibatch_size and buffer.length() > update_after:\n",
        "      optimizer.zero_grad()\n",
        "      # TODO: Sample a minibatch randomly\n",
        "      # ....\n",
        "      minibatch = buffer.sample_batch(batch_size=32)\n",
        "      # TODO: Compute q values for states\n",
        "      # ....\n",
        "      state_batch = torch.cat(minibatch.state)\n",
        "      action_batch = torch.cat(minibatch.action)\n",
        "      reward_batch = torch.cat(minibatch.reward)\n",
        "      q = dqn(state_batch).gather(1, action_batch)\n",
        "      # ....\n",
        "      # TODO: compute the targets for training\n",
        "      # ....\n",
        "      with torch.no_grad():\n",
        "        targets = dqn_target(next_state)\n",
        "      # TODO: compute the predictions for training\n",
        "      # ....\n",
        "      target_q = (targets * gamma) + reward_batch\n",
        "      # TODO: Compute loss: mse = mean squared error\n",
        "      loss = mse(q, target_q)\n",
        "      # print('predictions', predictions, 'targets', targets)\n",
        "      # print(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      # Update target network\n",
        "      soft_update(dqn, dqn_target, tau)\n",
        "    if done:\n",
        "      state = env.reset()[0]\n",
        "      print(f\"Episode: \\t{i}\\t{ret}\\t{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}\")\n",
        "      break\n",
        "  returns.append(ret)\n",
        "  returns_50.append(ret)\n",
        "  if i % 50 == 0:\n",
        "    store_checkpoint(checkpoint_path=save_path, dqn_net=dqn, timesteps=timesteps)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i, np.mean(returns_50)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOMr73xUpF1Q"
      },
      "outputs": [],
      "source": [
        "plot(timesteps, returns, losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frH7sAH8iNqK"
      },
      "source": [
        "Evaluate your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkdD-B8RqVW5"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = np.expand_dims(state, 0)\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
        "        with torch.no_grad():\n",
        "            q_action = self.model(state).detach().cpu().numpy()\n",
        "\n",
        "            return np.argmax(q_action)\n",
        "\n",
        "def run_episode(env, agent, seed=None):\n",
        "    state = env.reset(seed=seed)[0]\n",
        "    score = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.select_action(state)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        score += reward\n",
        "        done = terminated or truncated\n",
        "    env.close()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q05yTkf6qVUZ"
      },
      "outputs": [],
      "source": [
        "N_EPISODES = 50\n",
        "agent = Agent(model=dqn, device=device)\n",
        "\n",
        "scores = []\n",
        "for i in range(N_EPISODES):\n",
        "    seed = np.random.randint(1e7)\n",
        "    scores.append(run_episode(env, agent, seed=seed))\n",
        "\n",
        "# Print result\n",
        "print(\"Average Return:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF48ua4Qj5Ya"
      },
      "source": [
        "Convert model to ONNX for Submission:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDhiYU1WqVO8"
      },
      "outputs": [],
      "source": [
        "def save_as_onnx(torch_model, sample_input, model_path):\n",
        "    torch.onnx.export(torch_model,             # model being run\n",
        "                    sample_input,              # model input (or a tuple for multiple inputs)\n",
        "                    f=model_path,              # where to save the model (can be a file or file-like object)\n",
        "                    export_params=True,        # store the trained parameter weights inside the model file\n",
        "                    opset_version=17,          # the ONNX version to export the model to - see https://github.com/microsoft/onnxruntime/blob/master/docs/Versioning.md\n",
        "                    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMHELa0_qVMU"
      },
      "outputs": [],
      "source": [
        "sample_state = env.reset(seed=seed)[0]\n",
        "save_as_onnx(dqn, torch.tensor(sample_state, dtype=torch.float32, device=device), 'submission_model_new.onnx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZONcviQ8zyO"
      },
      "outputs": [],
      "source": [
        "# load the onnx model and test it:\n",
        "import onnx\n",
        "from onnx2pytorch import ConvertModel\n",
        "\n",
        "dqn = ConvertModel(onnx.load('submission_model_new.onnx'))\n",
        "dqn = dqn.to(device)\n",
        "dqn.eval()\n",
        "\n",
        "agent = Agent(model=dqn, device=device)\n",
        "\n",
        "scores = []\n",
        "for i in range(N_EPISODES):\n",
        "    seed = np.random.randint(1e7)\n",
        "    scores.append(run_episode(env, agent, seed=seed))\n",
        "\n",
        "\n",
        "print(\"Average Return:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi-v__FJufCb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
