{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "\n",
    "This material, no matter whether in printed or electronic form,\n",
    "may be used for personal and non-commercial educational use only.\n",
    "Any reproduction of this manuscript,\n",
    "no matter whether as a whole or in parts,\n",
    "no matter whether in printed or in electronic form,\n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 3 - SS 2024 -->\n",
    "\n",
    "# Monitoring, Hyperparameters and efficient CNNs  (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This notebook contains one of the assignments for the exercises in Deep Learning and Neural Nets 2.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility functions that should work without (too much) problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless explicitly allowed!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, the main goal is to get familiar with neural network hyperparameter search.\n",
    "More specifically, you will perform hyperparameter search on some real-world data.\n",
    "To prepare you for the search, we will first look at how you can monitor the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "torch.manual_seed(1806)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~\\.pytorch\n"
     ]
    }
   ],
   "source": [
    "# google colab data management\n",
    "import os.path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    _home = 'gdrive/MyDrive/'\n",
    "except ImportError:\n",
    "    _home = '~'\n",
    "finally:\n",
    "    data_root = os.path.join(_home, '.pytorch')\n",
    "\n",
    "print(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tracking Progress\n",
    "\n",
    "Training a deep neural network with millions of parameters can cost quite some time.\n",
    "E.g. Alexnet already requires roughly [225 hours][alexnet] (>1 week) of compute on a single GPU.\n",
    "In order to make sure that the network is training as expected,\n",
    "it is crucial to get some insights into how training progresses.\n",
    "After all, you do not want to waste hundreds of hours of compute to find out \n",
    "that training had already diverged in the first few minutes.\n",
    "Therefore it is important to be able to monitor the training process.\n",
    "\n",
    "[alexnet]: https://arxiv.org/abs/1404.5997\n",
    "\n",
    "As a matter of fact, the `update` and `evaluate` functions \n",
    "already implement some sort of ad hoc monitoring by providing the list of errors in a batch.\n",
    "This list can be used to print the mean loss after every epoch\n",
    "and can therefore be used to get an idea of how learning is progressing.\n",
    "This specific implementation of monitoring the loss is not very flexible, however,\n",
    "since it is not possible to access the information before the epoch has finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we start, we will tackle the flexibility of monitoring the loss\n",
    "by creating a separate `Tracker` class to keep track\n",
    "of important steps and results during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    \"\"\" Tracks useful information as learning progresses. \"\"\"\n",
    "\n",
    "    def __init__(self, *loggers: \"Logger\"):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        logger0, logger1, ... loggerN : Logger\n",
    "            One or more loggers for logging training information.\n",
    "        \"\"\"\n",
    "        self.epoch = 0\n",
    "        self.update = 0\n",
    "        self._tag = None\n",
    "        self._losses = []\n",
    "        self._summary = {}\n",
    "\n",
    "        self.loggers = list(loggers)\n",
    "\n",
    "    def start_epoch(self, count: bool = True):\n",
    "        \"\"\" Start one iteration of updates over the training data. \"\"\"\n",
    "        if count:\n",
    "            self.epoch += 1\n",
    "        \n",
    "        self._summary.clear()\n",
    "        for logger in self.loggers:\n",
    "            logger.on_epoch_start(self.epoch)\n",
    "\n",
    "    def end_epoch(self):\n",
    "        \"\"\" Wrap up one iteration of updates over the training data. \"\"\"\n",
    "        for logger in self.loggers:\n",
    "            logger.on_epoch_end(self.epoch, **self._summary)\n",
    "\n",
    "        return dict(self._summary)\n",
    "\n",
    "    def start(self, tag: str, num_batches: int = None):\n",
    "        \"\"\" Start a loop over mini-batches. \"\"\"\n",
    "        self._tag = tag\n",
    "        self._losses.clear()\n",
    "        for logger in self.loggers:\n",
    "            logger.on_iter_start(self.epoch, self.update, self._tag, num_steps_expected=num_batches)\n",
    "    \n",
    "    def step(self, loss: float):\n",
    "        \"\"\" Register the loss of a single mini-batch. \"\"\"\n",
    "        self._losses.append(loss)\n",
    "        for logger in self.loggers:\n",
    "            logger.on_iter_update(self.epoch, self.update, self._tag, loss=loss)  \n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\" Wrap up and summarise a loop over mini-batches. \"\"\"\n",
    "        losses = self._losses\n",
    "        avg_loss = float(\"nan\") if len(losses) == 0 else sum(losses) / len(losses)\n",
    "        self._summary[self._tag] = avg_loss\n",
    "        for logger in self.loggers:\n",
    "            logger.on_iter_end(self.epoch, self.update, self._tag, avg_loss=avg_loss)\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def count_update(self):\n",
    "        \"\"\" Increase the update counter. \"\"\"\n",
    "        self.update += 1\n",
    "        for logger in self.loggers:\n",
    "            logger.on_update(self.epoch, self.update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class provides the same functionality as the list that\n",
    "you might have used in the current `update` and `evaluate` functions.\n",
    "However, it also makes it possible to extend the functionality\n",
    "of both functions without the need to interfere with existing code.\n",
    "\n",
    "Note that there are libraries and frameworks out there that provide\n",
    "(parts of) the functionality we will implement in what follows.\n",
    "Two example frameworks that directly build on pytorch are\n",
    "[pytorch-lightning](https://www.pytorchlightning.ai/)\n",
    "and [pytorch ignite](https://pytorch.org/ignite/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 1: Combining Classes for Tracking (3 points)\n",
    "\n",
    "You might not have noticed yet, but in assignment 2, a `Trainer` class was introduced.\n",
    "The goal of this exercise is to extend this `Trainer` class to make use of the `Tracker`.\n",
    "\n",
    " > Update the `Trainer` class to make use of the `tracker` attribute (see `__init__`).\n",
    " > The functionality and outputs of the current implementation should be preserved.\n",
    " > Also, make sure to offload as much as possible to the `tracker`.\n",
    " > You will want to use every method of the `Tracker` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "542bc8041785782ba67644420216640c",
     "grade": false,
     "grade_id": "cell-55a83c02adbf3782",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\" Class to organise learning and monitoring. \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "         model: nn.Module,\n",
    "         criterion: nn.Module,\n",
    "         optimiser: optim.Optimizer,\n",
    "         tracker: Tracker = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torch.nn.Module\n",
    "            Neural Network that will be trained.\n",
    "        criterion : torch.nn.Module\n",
    "            Loss function to use for training.\n",
    "        optimiser : torch.optim.Optimizer\n",
    "            Optimisation strategy for training.\n",
    "        tracker : Tracker, optional\n",
    "            Tracker to keep track of training progress.\n",
    "        \"\"\"\n",
    "        if tracker is None:\n",
    "            tracker = Tracker()\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimiser = optimiser\n",
    "\n",
    "        self.tracker = tracker\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\" Current state of learning. \"\"\"\n",
    "        return {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"objective\": self.criterion.state_dict(),\n",
    "            \"optimiser\": self.optimiser.state_dict(),\n",
    "            \"num_epochs\": self.tracker.epoch,\n",
    "            \"num_updates\": self.tracker.update,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\" Device of the (first) model parameters. \"\"\"\n",
    "        return next(self.model.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, batches: DataLoader, tag: str = None):\n",
    "        \"\"\"\n",
    "        One epoch of evaluating the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batches : DataLoader\n",
    "            An iterator over mini-batches of data to use for updating.\n",
    "        tag : str, optional\n",
    "            Identification tag for tracking loss values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        avg_loss : float\n",
    "            The average loss over all mini-batches.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        device = self.device\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        self.tracker.start('valid', num_batches=batches.batch_size) # start the tracker for the evaluation \n",
    "        \n",
    "        losses = []\n",
    "        for x, y in batches:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = self.model(x)\n",
    "            loss = self.criterion(logits, y)\n",
    "            losses.append(loss.item())\n",
    "            self.tracker.step(loss.item()) # track the loss of one epoch\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        self.tracker.summary() # summary of the loop over the batches\n",
    "        self.tracker.count_update() # increase the update counter\n",
    "        return avg_loss\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def update(self, batches: DataLoader, tag: str = None):\n",
    "        \"\"\"\n",
    "        One epoch of updating the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batches : DataLoader\n",
    "            An iterator over mini-batches of data to use for updating.\n",
    "        tag : str, optional\n",
    "            Identification tag for tracking loss values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        avg_loss : float\n",
    "            The average loss over all mini-batches.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        device = self.device\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        self.tracker.start('train', num_batches=batches.batch_size) # start the tracker for the training epch\n",
    "        \n",
    "        losses = []\n",
    "        for x, y in batches:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = self.model(x)\n",
    "            loss = self.criterion(logits, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            self.tracker.step(loss=loss.item()) # track the loss of one epoch\n",
    "            \n",
    "            self.optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "\n",
    "        self.tracker.summary() # summary of the loop over the batches\n",
    "        self.tracker.count_update() # increase the update counter\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        return avg_loss\n",
    "\n",
    "    def train(self, train_batches, valid_batches=None, num_epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Train the network for multiple epochs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_batches : DataLoader\n",
    "            The training data for updating the network.\n",
    "        valid_batches : DataLoader, optional\n",
    "            The validation data for estimating the generalisation performance.\n",
    "        num_epochs : int, optional\n",
    "            The number of epochs to train.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        results : dict\n",
    "            The average loss estimates after `num_epochs` epochs.\n",
    "            \n",
    "        \"\"\"\n",
    "        if valid_batches is None:\n",
    "            valid_batches = ()\n",
    "\n",
    "        ##################These two lines make no sense for me, just putting them within the tracker ###\n",
    "        self.tracker.start_epoch(count=False)\n",
    "        train_loss = self.evaluate(train_batches)\n",
    "        valid_loss = self.evaluate(valid_batches)\n",
    "        self.tracker.end_epoch()\n",
    "        ##################################################\n",
    "        for _ in range(num_epochs):\n",
    "            self.tracker.start_epoch()\n",
    "            train_loss = self.update(train_batches)\n",
    "            valid_loss = self.evaluate(valid_batches)\n",
    "            self.tracker.end_epoch()\n",
    "        return {\"train\": train_loss, \"valid\": valid_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37164b7ab5b8642d2d86cc05179fbe7b",
     "grade": false,
     "grade_id": "cell-86e4485f6564b4f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sanity check (and test setup)\n",
    "from torchvision import transforms\n",
    "mean, std = .1307, .3081\n",
    "normalise = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean, ), (std, ))\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(data_root, train=False, transform=normalise, download=True)\n",
    "loader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=2)\n",
    "    \n",
    "conv_net = nn.Sequential(\n",
    "    nn.Conv2d(1, 8, 5), nn.MaxPool2d(3), nn.ELU(),\n",
    "    nn.Conv2d(8, 16, 7), nn.ELU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64, 10),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=conv_net.to(device),\n",
    "    criterion=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    optimiser=optim.Adam(conv_net.parameters(), lr=1e-2),\n",
    ")\n",
    "\n",
    "results = trainer.train(loader, loader)\n",
    "assert \"train\" in results, \"ex1: could not find training loss in results\"\n",
    "assert \"valid\" in results, \"ex1: could not find validation loss in results\"\n",
    "assert isinstance(results[\"train\"], float), (\n",
    "    f\"ex1: expected training loss to be of type 'float', but found '{type(results['train'])}'\"\n",
    ")\n",
    "assert isinstance(results[\"valid\"], float), (\n",
    "    f\"ex1: expected validation loss to be of type 'float', but found '{type(results['valid'])}'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8ad22fdbeee27de5553334215810027",
     "grade": true,
     "grade_id": "cell-eff7a8c10a8d86db",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1394f1b7673b9993b7858059fa309ec8",
     "grade": true,
     "grade_id": "cell-35e703f7644d970c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "trainer = Trainer(\n",
    "    model=conv_net.to(device),\n",
    "    criterion=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    optimiser=optim.Adam(conv_net.parameters(), lr=1e-2),\n",
    ")\n",
    "results = trainer.train(loader, loader, num_epochs=1)\n",
    "assert trainer.tracker.epoch == 1, (\n",
    "    f\"ex1: expected tracker to have counted 1 epoch, but found {trainer.tracker.epoch} \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0db24b6f50fff62d171f726c6fe14e48",
     "grade": true,
     "grade_id": "cell-12c40d5f5ab823ee",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert \"train\" in results, \"ex1: could not find training loss in results\"\n",
    "assert \"valid\" in results, \"ex1: could not find validation loss in results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "782d2cc9f2d24911a222b9ca02711080",
     "grade": true,
     "grade_id": "cell-2ea9dbffe78f2867",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "results = trainer.evaluate(loader, tag=\"extra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logging Tracked Information\n",
    "\n",
    "In its simplest form, a `Tracker` only keeps track of what happens in an epoch.\n",
    "It knows about the loss values for each mini-batch,\n",
    "but also how many epochs and updates already happened.\n",
    "However, as mentioned earlier, a lot of features can be added to the `Tracker`.\n",
    "\n",
    "Most notably, we can use the `Tracker` to store certain information during training.\n",
    "Thus far, loss information has been collected to compute the average and is then discarded.\n",
    "In order to revisit this information later, it can be written to a file, or _logged_.\n",
    "\n",
    "For this purpose, we will use the interface provided by the `Logger` class (below).\n",
    "This way, different types of information can be logged in a flexible way.\n",
    "Luckily the `Tracker` class already provides everything that is necessary\n",
    "to work with loggers to monitor whatever we need during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \"\"\" Extracts and/or persists tracker information. \"\"\"\n",
    "\n",
    "    def __init__(self, path: str = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str or Path, optional\n",
    "            Path to where data will be stored.\n",
    "        \"\"\"\n",
    "        path = Path(\"run\") if path is None else Path(path)\n",
    "        self.path = path.expanduser().resolve()\n",
    "\n",
    "    def on_epoch_start(self, epoch: int, **kwargs):\n",
    "        \"\"\"Actions to take on the start of an epoch.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, **kwargs):\n",
    "        \"\"\"Actions to take on the end of an epoch.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_iter_start(self, epoch: int, update: int, tag: str, **kwargs):\n",
    "        \"\"\"Actions to take on the start of an iteration.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_iter_update(self, epoch: int, update: int, tag: str, **kwargs):\n",
    "        \"\"\"Actions to take when an update has occurred.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_iter_end(self, epoch: int, update: int, tag: str, **kwargs):\n",
    "        \"\"\"Actions to take on the end of an iteration.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def on_update(self, epoch: int, update: int):\n",
    "        \"\"\"Actions to take when the model is updated.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Progress bar (1 point)\n",
    "\n",
    "Monitoring the loss early on during training can be useful\n",
    "to check whether things are working as expected.\n",
    "In combination with an indication of progress in training,\n",
    "expectations can be properly managed early on.\n",
    "\n",
    " > Create a logger that produces some sort of progress bar for each epoch.\n",
    " > The progress bar should show the current epoch, the current trainnig stage (tag) and the current loss value.\n",
    " > Moreover, it should print a short summary after each epoch, including the average loss for each tag.\n",
    " > Note that most of this information is passed through the `kwargs` in the `Logger` methods.\n",
    "\n",
    "**Hint:** You probably want to make use of the [`tqdm` library](https://tqdm.github.io/docs/tqdm/) to manage the progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e0097ecb5682bd4f63e4e2d0d4e3cf0",
     "grade": false,
     "grade_id": "cell-b869846d2cbdf39a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProgressBar(Logger):\n",
    "    \"\"\"Log progress of epoch using a progress bar.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.progress_bar = None\n",
    "        self.summary = {}\n",
    "\n",
    "\n",
    "    def on_epoch_start(self, epoch, **kwargs):\n",
    "        self.progress_bar = tqdm(total = 100, desc=f'Epoch {epoch}', position=0, leave=True)\n",
    "        self.summary.clear()\n",
    "\n",
    "    def on_iter_update(self, epoch: int, update: int, tag: str, loss: float, **kwargs):\n",
    "        self.progress_bar.set_postfix_str(f'{tag} Epoch {epoch}, Loss: {loss}')\n",
    "        self.progress_bar.update(update)\n",
    "        if tag not in self.summary:\n",
    "            self.summary[tag] = []\n",
    "        self.summary[tag].append(loss)\n",
    "\n",
    "    def on_epoch_end(self, epoch, **kwargs):\n",
    "        summary_print = ''\n",
    "        for tag, loss in self.summary.items():\n",
    "            avg_loss = sum(loss) / len(loss)\n",
    "            summary_print += (f'{tag} epoch: Average Loss {avg_loss} |')\n",
    "        print(f'Summary for Epoch {epoch}: {summary_print}')\n",
    "        self.progress_bar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7616607133bf53ef1c5930cccdbdac5a",
     "grade": false,
     "grade_id": "cell-66efa3659d3d818f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c545ea86c84145aea9e5bffe39e56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Epoch 11: eval epoch: Average Loss 367.96900329589846 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48aad0e8f9034ceaa25ce5746f32c155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Epoch 12: train epoch: Average Loss 365.26734619140626 |eval epoch: Average Loss 352.99444580078125 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6777fafe2ea948319222590bd7510e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Epoch 13: train epoch: Average Loss 354.84987182617186 |eval epoch: Average Loss 351.3937454223633 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9c580b67f24e9daccd0a2f00587ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Epoch 14: train epoch: Average Loss 341.0695068359375 |eval epoch: Average Loss 331.2452087402344 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b432b70550714cddad74fb3e9f8ecc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Epoch 15: train epoch: Average Loss 333.4515411376953 |eval epoch: Average Loss 330.83526611328125 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfef342c52ac4b5dae86fb55f3ccd681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Epoch 16: train epoch: Average Loss 329.46788482666017 |eval epoch: Average Loss 328.9459167480469 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 329.46788482666017, 'valid': 328.9459167480469}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check (and test setup)\n",
    "progress = ProgressBar()\n",
    "trainer.tracker.loggers = [progress]\n",
    "trainer.train(loader, loader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd370adb4554bfbdac4834a54ec04022",
     "grade": true,
     "grade_id": "cell-68501da29483d4fd",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de2d3b97ab820dcc29895581ff94998a",
     "grade": true,
     "grade_id": "cell-00b8ce97941f4463",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 3: Tensorboard (2 points)\n",
    "\n",
    "[Tensorboard](https://www.tensorflow.org/tensorboard) \n",
    "is a library that allows to track and visualise data during and after training.\n",
    "Apart from scalar metrics, tensorboard can process distributions, images and much more.\n",
    "It started as a part of tensorflow, but was then made available as a standalone library.\n",
    "This makes it possible to use tensorboard for visualising pytorch data.\n",
    "As a matter of fact, tensorboard is readily available in pytorch.\n",
    "From [`torch.utils.tensorboard`](https://pytorch.org/docs/stable/tensorboard.html),\n",
    "the `SummaryWriter` class can be used to track various types of data.\n",
    "\n",
    " > Create a Logger that makes use of the `Summarywriter` to monitor the loss with tensorboard.\n",
    " > On one hand, it should monitor the loss for every batch and both modes using `<tag>/loss` as tag.\n",
    " > On the other hand, it should monitor the average losses after every stage, using `'<tag>/avg_loss'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3765fb47f0d99f64c39be3539f8cba0a",
     "grade": false,
     "grade_id": "cell-a4a677885f648eae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TensorBoard(Logger):\n",
    "    \"\"\"Log loss values to tensorboard.\"\"\"\n",
    "\n",
    "    def __init__(self, path: Path = None, every: int = 1):\n",
    "        super().__init__(path)\n",
    "        self.every = every\n",
    "        # YOUR CODE HERE\n",
    "        self.writer = SummaryWriter(log_dir=self.path)\n",
    "\n",
    "    def on_iter_update(self, epoch, update, tag, loss):\n",
    "        tag = f'{tag}/loss'\n",
    "        self.writer.add_scalar(tag, loss, epoch)\n",
    "\n",
    "    def on_iter_end(self, epoch, update, tag, avg_loss):\n",
    "        tag = f'{tag}/avg_loss'\n",
    "        self.writer.add_scalar(tag, avg_loss, epoch)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ac5c12a8ad1618b7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ac5c12a8ad1618b7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run in the following in the command line if it does not work in jupyter\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "866731e2a917ce26b0dbc9977bfe9096",
     "grade": false,
     "grade_id": "cell-54e40dfc4962b0ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 208.1708190917969, 'valid': 213.68511199951172}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check (and test setup)\n",
    "tb = TensorBoard()\n",
    "trainer.tracker.loggers = [tb]\n",
    "trainer.train(loader, loader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ad1b323eb7f4b75446c340704899d55",
     "grade": true,
     "grade_id": "cell-3dbb7c2b800c3974",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ex3: could not find validation loss",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m tags = tb_data.Tags()[\u001b[33m\"\u001b[39m\u001b[33mscalars\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtrain/loss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tags, \u001b[33m\"\u001b[39m\u001b[33mex3: could not find training loss\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalid/loss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tags, \u001b[33m\"\u001b[39m\u001b[33mex3: could not find validation loss\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: ex3: could not find validation loss"
     ]
    }
   ],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "path = next(tb.path.glob(\"events.out.tfevents.*\"))\n",
    "tb_data = EventAccumulator(str(path)).Reload()\n",
    "tags = tb_data.Tags()[\"scalars\"]\n",
    "\n",
    "assert \"train/loss\" in tags, \"ex3: could not find training loss\"\n",
    "assert \"valid/loss\" in tags, \"ex3: could not find validation loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a14e79e0093401fb7e959b4c4c1a0f64",
     "grade": true,
     "grade_id": "cell-09f4d19e411461ad",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test Cell: do not edit or delete!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/avg_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtags\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex3: could not find avg training loss\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid/avg_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tags, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex3: could not find avg validation loss\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tags' is not defined"
     ]
    }
   ],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert \"train/avg_loss\" in tags, \"ex3: could not find avg training loss\"\n",
    "assert \"valid/avg_loss\" in tags, \"ex3: could not find avg validation loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Always have a Backup-plan (1 point)\n",
    "\n",
    "Apart from logging metrics like e.g. loss and accuracy,\n",
    "it can often be useful to create a backup (or checkpoint) of training progress.\n",
    "After all, you do not want hours of training to get lost\n",
    "due to a programming error in a print statement at the end of your code.\n",
    "This idea can also be useful to implement some form of early-stopping.\n",
    "However, we will ignore that for now.\n",
    "\n",
    " > Implement a logger that saves the state of the trainer every few epochs.\n",
    " > For the sake of convention, use the `.pth` extension for storing these backups.\n",
    "\n",
    "**Hint:** you may want to raise a [`warning`](https://docs.python.org/3/library/warnings.html#available-functions) if no trainer has been attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fda52bc56679f40ca0589153ca2ebeac",
     "grade": false,
     "grade_id": "cell-7d2d4f754671f37f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Backup(Logger):\n",
    "    \n",
    "    DEFAULT_FILE = \"backup.pth\"\n",
    "    \n",
    "    def __init__(self, path: Path = None, every: int = 1):\n",
    "        super().__init__(path)\n",
    "        self.trainer = None\n",
    "        self.every = every\n",
    "        \n",
    "        if self.path.is_dir() or not self.path.suffix:\n",
    "            self.path = self.path / self.DEFAULT_FILE\n",
    "        \n",
    "        self.path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    def attach_trainer(self, trainer: Trainer):\n",
    "        self.trainer = trainer\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    def on_iter_end(self, epoch, update, tag, avg_loss):\n",
    "        if epoch % self.every == 0:\n",
    "            print(f\"Backup at epoch {epoch}\")\n",
    "            if self.trainer is not None:\n",
    "                torch.save(self.trainer.state_dict(), self.path)\n",
    "            else:\n",
    "                print(\"No trainer attached to the logger\")\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d74df48ebd2b76e8daff96986fffee02",
     "grade": false,
     "grade_id": "cell-22bc3002500b39f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup at epoch 12\n",
      "Backup at epoch 12\n",
      "Backup at epoch 14\n",
      "Backup at epoch 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check (and test setup)\n",
    "checkpoints = Backup(every=2)\n",
    "trainer.tracker.loggers = [checkpoints]\n",
    "checkpoints.attach_trainer(trainer)\n",
    "trainer.train(loader, loader, num_epochs=4)\n",
    "trainer.tracker.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c0be6370d40bc7c56087a010d00c65e",
     "grade": true,
     "grade_id": "cell-24a856bc097f4844",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "print(torch.load(checkpoints.path)[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up checkpoints and tensorboard logs\n",
    "! rm -r run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "Finding good hyperparameters for a model is a general problem in machine learning (or even statistics).\n",
    "However, neural networks are (in)famous for their large number of hyperparameters.\n",
    "To list a few: learning rate, batch size, epochs, pre-processing, layer count, neurons for each layer, \n",
    "activation function, initialisation, normalisation, layer type, skip connections, regularisation, ...\n",
    "Moreover, it is often not possible to theoretically justify a particular choice for a hyperparameter.\n",
    "E.g. there is no way to tell whether $N$ or $N + 1$ neurons in a layer would be better, without trying it out.\n",
    "Therefore, hyperparameter search for neural networks is an especially tricky problem to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Manual Search\n",
    "\n",
    "The most straightforward approach to finding good hyperparameters is to just \n",
    "try out *reasonable* combinations of hyperparameters and pick the best model (using e.g. the validation set).\n",
    "The first problem with this approach is that it requires a gut feeling as to what *reasonable* combinations are.\n",
    "Moreover, it is often unclear how different hyperparameters interact with each other,\n",
    "which can make an irrelevant hyperparameter look more important than it actually is or vice versa.\n",
    "Finally, manual hyperparameter search is time consuming, since it is generally not possible to automate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Grid Search\n",
    "\n",
    "Getting a feeling for combinations of hyperparameters is often much harder than for individual hyperparameters.\n",
    "The idea of grid search is to get a set of *reasonable* values for each hyperparameter individually\n",
    "and organise these sets in a grid that represents all possible combinations of these values.\n",
    "Each combinations of hyperparameters in the grid can then be run simultaneously,\n",
    "assuming that so much hardware is available, which can speed up the search significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Search\n",
    "\n",
    "Since there are plenty of hyperparameters and each hyperparameters can have multiple *reasonable* values,\n",
    "it is often not feasible to try out every possible combination in the grid.\n",
    "On top of that, most of the models will be thrown away anyway because only the best model is of interest,\n",
    "even though they might achieve similar performance.\n",
    "The idea of random search is to randomly sample configurations, rather than choosing from pre-defined choices.\n",
    "This can be interpreted as setting up an infinite grid and trying only a few --- rather than all --- possibilities.\n",
    "Under the assumption that there are a lot of configurations with similarly good performance as the best model,\n",
    "this should provide a model that performs very good with high probability for a fraction of the compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bayesian Optimisation \n",
    "\n",
    "Rather than picking configurations completely at random, \n",
    "it is also possible to guide the random search.\n",
    "This is essentially the premise of Bayesian optimisation:\n",
    "sample inputs and evaluate the objective to find which parameters are likely to give good performance.\n",
    "\n",
    "Bayesian optimisation uses a function approximator for the objective \n",
    "and what is known as an *acquisition* function.\n",
    "The function approximator, or *surrogate*, \n",
    "has to be able to model a distribution over function values, e.g. a Gaussian Process.\n",
    "The acquisition function then uses these distributions\n",
    "to find where the largest improvements can be made, e.g. using the cdf.\n",
    "For a more elaborate explanation of Bayesian optimisation, \n",
    "see e.g. [this tutorial](https://arxiv.org/abs/1807.02811)\n",
    "\n",
    "This approach is less parallellisable than grid or random search,\n",
    "since it uses the information from previous runs to find good sampling regions.\n",
    "However, often there are more configurations to be tried out than there are computing devices\n",
    "and it is still possible to sample multiple configurations at each step with Bayesian Optimisation.\n",
    "Also consider [this paper](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms) in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Neural Architecture Search\n",
    "\n",
    "Instead of using Bayesian optimisation, \n",
    "the problem of hyperparameter search can also be tackled by other optimisation algorithms.\n",
    "This approach is also known as *Neural Architecture Search* (NAS).\n",
    "There are different optimisation strategies that can be used for NAS,\n",
    "but the most common are evolutionary algorithms and (deep) reinforcement learning.\n",
    "Consider reading [this survey](http://jmlr.org/papers/v20/18-598.html) \n",
    "to get an overview of how NAS can be used to construct neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient CNNs\n",
    "\n",
    "In recent times CNNs have become more computationally efficient. Traditional convolutional layers apply filters across the entire depth of the input volume, mixing all the input channels to produce a single output channel. Depthwise separable convolutions, introduced as a key innovation in architectures like Xception, are a more efficient variant of the standard convolution operation. This process is divided into two layers: the depthwise convolution and the pointwise convolution. In the depthwise convolution, a single filter is applied per input channel, which significantly reduces the computational cost. Following this, a 1x1 convolution (pointwise convolution) is applied to combine the outputs of the depthwise layer, creating a new set of feature maps. This approach drastically reduces the number of parameters and computations, making the network more efficient and faster, which is especially beneficial for mobile and embedded devices.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/358585116/figure/fig1/AS:1127546112487425@1645839350616/Depthwise-separable-convolutions.png\" />\n",
    "\n",
    "Squeeze-and-Excitation layers introduce an additional level of adaptivity in CNNs, enabling the network to perform dynamic channel-wise feature recalibration. Squeeze-and-Exitation blocks are usually executed after a convolutional layer or block\n",
    "and before the residual connection by a series of relatively inexpensive computations\n",
    "\n",
    "1. A three dimensional input consisting of different channels and the two spati l\n",
    "dimensions is compressed into one dimension by global aver ge pooling. As a res lt\n",
    "the spatial information is squeezed into one descriptor per channel.\n",
    "2. The squeezed data is transformed by a two layer feed-forward neural network.  fter\n",
    "the first linear layer ReLU is used as activation functi n and after the se ond a\n",
    "sigmoid function is applied. This normalizes the output between 0 and 1 and can be\n",
    "interpreted as the significance per channel.\n",
    "3. The result is used to scale the input of the Squeeze-and-Exitation block by an element-\n",
    "wise multiplication.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*bmObF5Tibc58iE9iOu327w.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Create an efficient CNN (4 points)\n",
    "\n",
    "Today, neural networks frequently have millions or billions of parameters. However, CNNs have become more computationally efficient over the years. How far can you get with a limited amount of compute?\n",
    "\n",
    "> Create an efficient CNN with less than 30.000 parameters.\n",
    "> Use at least one depthwise separable or groupwise convolution or apply at least one squeeze-and-exitation layer after a convolution.\n",
    "\n",
    "Hint: Skip-connections and Normalization layers are frequently used to stabilize the training behavoir of deep CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "502ba802639aa90d04244c4a0e67bf58",
     "grade": false,
     "grade_id": "cell-02e789300e7a7c38",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class EfficientCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, conv1_channels=8, conv2_channels=16):\n",
    "        super(EfficientCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, conv1_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv1_channels, conv2_channels, 3, padding=1, groups=conv1_channels, bias=False),\n",
    "            nn.BatchNorm2d(conv2_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv2_channels * 16 * 16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:  41370\n"
     ]
    }
   ],
   "source": [
    "# sanity-check\n",
    "model = EfficientCNN(in_channels=3, num_classes=10)\n",
    "model(torch.zeros((1, 3, 32, 32)))\n",
    "print(\"number of parameters: \", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d7d6e63fb05da5fdcf8bb7f66b3a1ba",
     "grade": true,
     "grade_id": "cell-5fa4e008f0651dbe",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a925500a9b0196f6f641a8f334d1312a",
     "grade": true,
     "grade_id": "cell-ea9663f055ba266c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Training (4 points)\n",
    "\n",
    "In order to get a feeling for hyperparameter search, you have to try it out on some example. You can use the monitoring tools from previous exercises to log performance and get a feeling for which hyperparameters work well. \n",
    "\n",
    "> Train your EfficientCNN on CIFAR10 using the Trainer class. Use hyperparameter search for the learning rate, optimizer and maybe even the model architecture to get a CrossEntropyLoss < 1.5 within 10 epochs of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "serching for the best learning rate\n",
      "testing wit learning rate:  0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest learning rate is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with a loss of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_lr\n\u001b[0;32m---> 33\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[43mlr_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_trails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     37\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[0;32mIn[33], line 22\u001b[0m, in \u001b[0;36mlr_search\u001b[0;34m(num_trails)\u001b[0m\n\u001b[1;32m     19\u001b[0m citerion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, criterion, optimizer)\n\u001b[0;32m---> 22\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation loss for testing with learing rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 151\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_batches, valid_batches, num_epochs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mstart_epoch()\n\u001b[0;32m--> 151\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(valid_batches)\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mend_epoch()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m, in \u001b[0;36mTrainer.update\u001b[0;34m(self, batches, tag)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mstep(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# track the loss of one epoch\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39msummary() \u001b[38;5;66;03m# summary of the loop over the batches\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Cell for Hyperparameter search, you can freely edit or delete this code\n",
    "train_dataset = torchvision.datasets.CIFAR10(data_root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(data_root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=True, num_workers=2)\n",
    "model = EfficientCNN(in_channels=3, num_classes=10)\n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "def lr_search(num_trails = 10):\n",
    "    print('serching for the best learning rate')\n",
    "    for _ in range(num_trails):\n",
    "        best_lr = None\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        lr = random.choice(learning_rates)\n",
    "        print('testing wit learning rate: ', lr)\n",
    "        model = EfficientCNN(in_channels=3, num_classes=10)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        citerion = nn.CrossEntropyLoss()\n",
    "        trainer = Trainer(model, criterion, optimizer)\n",
    "\n",
    "        result = trainer.train(train_loader, test_loader, num_epochs = 10)\n",
    "        valid_loss = result['valid']\n",
    "        print(f'Validation loss for testing with learing rate {lr}: {valid_loss}')\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_lr = lr\n",
    "    print(f'Best learning rate is {best_lr} with a loss of {best_loss}')\n",
    "    return best_lr\n",
    "\n",
    "\n",
    "\n",
    "lr = lr_search(num_trails=10)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainer = Trainer(model, \n",
    "                  criterion, \n",
    "                  optimizer)\n",
    "results = trainer.train(train_loader, test_loader, num_epochs=10)\n",
    "validation_loss = results.get('valid')\n",
    "train_loss = results.get('train')\n",
    "print(f'Finished Training with: validation loss: {validation_loss}, train loss: {train_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2817577600479126\n"
     ]
    }
   ],
   "source": [
    "print(trainer.tracker.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2770), started 1:02:33 ago. (Use '!kill 2770' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-899a8704764429a5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-899a8704764429a5\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "750d12b1a0036464acd2628e2c8f6150",
     "grade": true,
     "grade_id": "cell-1e6b60003072171f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
